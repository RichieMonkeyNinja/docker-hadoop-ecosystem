{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5c364c-36ae-4b38-9604-33d7f6777502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bronze-spark-upload-hdfs\") \\\n",
    "    .config(\"spark.driver.host\", \"spark-notebook\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099154de-fa0c-4bb6-babc-b2af3ed78c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-03-19/listings.parquet', '/home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-06-24/listings.parquet', '/home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-09-26/listings.parquet', '/home/jovyan/work/bronze_layer/listings/city=edinburgh/extraction_date=2025-09-21/listings.parquet', '/home/jovyan/work/bronze_layer/listings/city=london/extraction_date=2025-09-14/listings.parquet']\n",
      "File: /home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-03-19/listings.parquet | Columns: 81 | Price Type: string\n",
      "File: /home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-06-24/listings.parquet | Columns: 81 | Price Type: string\n",
      "File: /home/jovyan/work/bronze_layer/listings/city=bristol/extraction_date=2025-09-26/listings.parquet | Columns: 20 | Price Type: double\n",
      "File: /home/jovyan/work/bronze_layer/listings/city=edinburgh/extraction_date=2025-09-21/listings.parquet | Columns: 20 | Price Type: double\n",
      "File: /home/jovyan/work/bronze_layer/listings/city=london/extraction_date=2025-09-14/listings.parquet | Columns: 20 | Price Type: double\n",
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "| 70.0|\n",
      "|149.0|\n",
      "|411.0|\n",
      "| NULL|\n",
      "|210.0|\n",
      "|280.0|\n",
      "| 90.0|\n",
      "| 61.0|\n",
      "|340.0|\n",
      "| 49.0|\n",
      "| NULL|\n",
      "|213.0|\n",
      "| NULL|\n",
      "| 96.0|\n",
      "| NULL|\n",
      "| 71.0|\n",
      "| NULL|\n",
      "| 48.0|\n",
      "| NULL|\n",
      "| 76.0|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "path = \"/home/jovyan/work/bronze_layer/listings/**/*.parquet\"\n",
    "files = glob.glob(path, recursive=True)\n",
    "print(files)\n",
    "\n",
    "for f in files:\n",
    "    file_path = os.path.join(path, f)\n",
    "    # Read just the schema of each file individually\n",
    "    temp_df = spark.read.parquet(f\"file://{file_path}\")\n",
    "    col_count = len(temp_df.columns)\n",
    "    price_type = dict(temp_df.dtypes).get('price')\n",
    "    \n",
    "    print(f\"File: {f} | Columns: {col_count} | Price Type: {price_type}\")\n",
    "\n",
    "df_test = spark.read.parquet('file:///home/jovyan/work/bronze_layer/listings/city=london/extraction_date=2025-09-14/listings.parquet')\n",
    "df_test.select(('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9869c86-1c1f-41fe-b964-e1a9efb4f617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from file:///home/jovyan/work/bronze_layer/listings...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.parquet.\n: org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\nInitial schema:\n\"STRUCT<id: BIGINT, listing_url: STRING, scrape_id: BIGINT, last_scraped: STRING, source: STRING, name: STRING, description: STRING, neighborhood_overview: STRING, picture_url: STRING, host_id: BIGINT, host_url: STRING, host_name: STRING, host_since: STRING, host_location: STRING, host_about: STRING, host_response_time: STRING, host_response_rate: STRING, host_acceptance_rate: STRING, host_is_superhost: STRING, host_thumbnail_url: STRING, host_picture_url: STRING, host_neighbourhood: STRING, host_listings_count: DOUBLE, host_total_listings_count: DOUBLE, host_verifications: STRING, host_has_profile_pic: STRING, host_identity_verified: STRING, neighbourhood: STRING, neighbourhood_cleansed: STRING, neighbourhood_group_cleansed: DOUBLE, latitude: DOUBLE, longitude: DOUBLE, property_type: STRING, room_type: STRING, accommodates: BIGINT, bathrooms: DOUBLE, bathrooms_text: STRING, bedrooms: DOUBLE, beds: DOUBLE, amenities: STRING, price: STRING, minimum_nights: BIGINT, maximum_nights: BIGINT, minimum_minimum_nights: DOUBLE, maximum_minimum_nights: DOUBLE, minimum_maximum_nights: DOUBLE, maximum_maximum_nights: DOUBLE, minimum_nights_avg_ntm: DOUBLE, maximum_nights_avg_ntm: DOUBLE, calendar_updated: DOUBLE, has_availability: STRING, availability_30: BIGINT, availability_60: BIGINT, availability_90: BIGINT, availability_365: BIGINT, calendar_last_scraped: STRING, number_of_reviews: BIGINT, number_of_reviews_ltm: BIGINT, number_of_reviews_l30d: BIGINT, availability_eoy: BIGINT, number_of_reviews_ly: BIGINT, estimated_occupancy_l365d: BIGINT, estimated_revenue_l365d: DOUBLE, first_review: STRING, last_review: STRING, review_scores_rating: DOUBLE, review_scores_accuracy: DOUBLE, review_scores_cleanliness: DOUBLE, review_scores_checkin: DOUBLE, review_scores_communication: DOUBLE, review_scores_location: DOUBLE, review_scores_value: DOUBLE, license: STRING, instant_bookable: STRING, calculated_host_listings_count: BIGINT, calculated_host_listings_count_entire_homes: BIGINT, calculated_host_listings_count_private_rooms: BIGINT, calculated_host_listings_count_shared_rooms: BIGINT, reviews_per_month: DOUBLE, city: STRING, extraction_date: STRING>\"\nSchema that cannot be merged with the initial schema:\n\"STRUCT<id: BIGINT, name: STRING, host_id: BIGINT, host_name: STRING, neighbourhood_group: DOUBLE, neighbourhood: STRING, latitude: DOUBLE, longitude: DOUBLE, room_type: STRING, price: DOUBLE, minimum_nights: BIGINT, number_of_reviews: BIGINT, last_review: STRING, reviews_per_month: DOUBLE, calculated_host_listings_count: BIGINT, availability_365: BIGINT, number_of_reviews_ltm: BIGINT, license: STRING, city: STRING, extraction_date: STRING>\".\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5$adapted(SchemaMergeUtils.scala:100)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:100)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"STRING\" and \"DOUBLE\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:582)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:574)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:571)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:571)\n\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:619)\n\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:565)\n\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:480)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:102)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df_listings = spark.read.parquet(local_path).option(\"mergeSchema\", \"true\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_listings \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:///home/jovyan/work/bronze_layer/listings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.parquet.\n: org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\nInitial schema:\n\"STRUCT<id: BIGINT, listing_url: STRING, scrape_id: BIGINT, last_scraped: STRING, source: STRING, name: STRING, description: STRING, neighborhood_overview: STRING, picture_url: STRING, host_id: BIGINT, host_url: STRING, host_name: STRING, host_since: STRING, host_location: STRING, host_about: STRING, host_response_time: STRING, host_response_rate: STRING, host_acceptance_rate: STRING, host_is_superhost: STRING, host_thumbnail_url: STRING, host_picture_url: STRING, host_neighbourhood: STRING, host_listings_count: DOUBLE, host_total_listings_count: DOUBLE, host_verifications: STRING, host_has_profile_pic: STRING, host_identity_verified: STRING, neighbourhood: STRING, neighbourhood_cleansed: STRING, neighbourhood_group_cleansed: DOUBLE, latitude: DOUBLE, longitude: DOUBLE, property_type: STRING, room_type: STRING, accommodates: BIGINT, bathrooms: DOUBLE, bathrooms_text: STRING, bedrooms: DOUBLE, beds: DOUBLE, amenities: STRING, price: STRING, minimum_nights: BIGINT, maximum_nights: BIGINT, minimum_minimum_nights: DOUBLE, maximum_minimum_nights: DOUBLE, minimum_maximum_nights: DOUBLE, maximum_maximum_nights: DOUBLE, minimum_nights_avg_ntm: DOUBLE, maximum_nights_avg_ntm: DOUBLE, calendar_updated: DOUBLE, has_availability: STRING, availability_30: BIGINT, availability_60: BIGINT, availability_90: BIGINT, availability_365: BIGINT, calendar_last_scraped: STRING, number_of_reviews: BIGINT, number_of_reviews_ltm: BIGINT, number_of_reviews_l30d: BIGINT, availability_eoy: BIGINT, number_of_reviews_ly: BIGINT, estimated_occupancy_l365d: BIGINT, estimated_revenue_l365d: DOUBLE, first_review: STRING, last_review: STRING, review_scores_rating: DOUBLE, review_scores_accuracy: DOUBLE, review_scores_cleanliness: DOUBLE, review_scores_checkin: DOUBLE, review_scores_communication: DOUBLE, review_scores_location: DOUBLE, review_scores_value: DOUBLE, license: STRING, instant_bookable: STRING, calculated_host_listings_count: BIGINT, calculated_host_listings_count_entire_homes: BIGINT, calculated_host_listings_count_private_rooms: BIGINT, calculated_host_listings_count_shared_rooms: BIGINT, reviews_per_month: DOUBLE, city: STRING, extraction_date: STRING>\"\nSchema that cannot be merged with the initial schema:\n\"STRUCT<id: BIGINT, name: STRING, host_id: BIGINT, host_name: STRING, neighbourhood_group: DOUBLE, neighbourhood: STRING, latitude: DOUBLE, longitude: DOUBLE, room_type: STRING, price: DOUBLE, minimum_nights: BIGINT, number_of_reviews: BIGINT, last_review: STRING, reviews_per_month: DOUBLE, calculated_host_listings_count: BIGINT, availability_365: BIGINT, number_of_reviews_ltm: BIGINT, license: STRING, city: STRING, extraction_date: STRING>\".\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5$adapted(SchemaMergeUtils.scala:100)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:100)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"STRING\" and \"DOUBLE\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:582)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:574)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:571)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:571)\n\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:619)\n\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:565)\n\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:480)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:102)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "local_path = \"file:///home/jovyan/work/bronze_layer/listings\"\n",
    "\n",
    "print(f\"Reading data from {local_path}...\")\n",
    "# df_listings = spark.read.parquet(local_path).option(\"mergeSchema\", \"true\")\n",
    "df_listings = spark.read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .parquet(\"file:///home/jovyan/work/bronze_layer/listings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8f6b4e-0ce9-411d-a854-c063d54f7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+------+---------------+\n",
      "|   id|                name|  price|  city|extraction_date|\n",
      "+-----+--------------------+-------+------+---------------+\n",
      "|13913|Holiday London DB...| $70.00|london|     2025-09-14|\n",
      "|15400|Bright Chelsea  A...|$149.00|london|     2025-09-14|\n",
      "|17402|Very Central Mode...|$411.00|london|     2025-09-14|\n",
      "|24328|Battersea live/wo...|   NULL|london|     2025-09-14|\n",
      "|36274|Bright 1 bedroom ...|$210.00|london|     2025-09-14|\n",
      "|36299|Kew Gardens 3BR h...|$280.00|london|     2025-09-14|\n",
      "|36660|You are GUARANTEE...| $90.00|london|     2025-09-14|\n",
      "|38605|SUNNY ROOM PRIVAT...| $61.00|london|     2025-09-14|\n",
      "|38610|     Short Term Home|$340.00|london|     2025-09-14|\n",
      "|38995|SPACIOUS ROOM IN ...| $49.00|london|     2025-09-14|\n",
      "|39387|Stylish bedsit in...|   NULL|london|     2025-09-14|\n",
      "|41445|2 Double bed apar...|$213.00|london|     2025-09-14|\n",
      "|41509|Room in maisonett...|   NULL|london|     2025-09-14|\n",
      "|41712|Room with a view,...| $96.00|london|     2025-09-14|\n",
      "|41870|Room in relaxed f...|   NULL|london|     2025-09-14|\n",
      "|42010|You Will Save Mon...| $71.00|london|     2025-09-14|\n",
      "|42692|Fabulous flat w g...|   NULL|london|     2025-09-14|\n",
      "|43129|Quiet Comfortable...| $48.00|london|     2025-09-14|\n",
      "|43202|Beautiful 1 bed a...|   NULL|london|     2025-09-14|\n",
      "|45163|  Room with a garden| $76.00|london|     2025-09-14|\n",
      "+-----+--------------------+-------+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_listings.select(\"id\", \"name\", \"price\", \"city\", \"extraction_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c442e0b-6741-4fac-b6ca-183f12570c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings = df_listings.withColumn('updated_at_utc+0', F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bcdc89-3a8c-4349-9f32-93357fe90378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingestion complete. Data stored at: hdfs://namenode:9000/user/hive/warehouse/airbnb.db/bronze/listings\n"
     ]
    }
   ],
   "source": [
    "hdfs_destination = \"hdfs://namenode:9000/user/hive/warehouse/airbnb.db/bronze/listings\"\n",
    "\n",
    "(df_listings.write\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"extraction_date\", \"city\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(hdfs_destination))\n",
    "\n",
    "print(f\"✅ Ingestion complete. Data stored at: {hdfs_destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1c47b9-e998-4236-b988-013669bdfd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the Database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS airbnb_bronze\")\n",
    "\n",
    "# 2. Create the Table\n",
    "spark.sql('''\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS airbnb_bronze.listings (\n",
    "    id BIGINT,\n",
    "    listing_url STRING,\n",
    "    scrape_id BIGINT,\n",
    "    last_scraped STRING,\n",
    "    source STRING,\n",
    "    name STRING,\n",
    "    description STRING,\n",
    "    neighborhood_overview STRING,\n",
    "    picture_url STRING,\n",
    "    host_id BIGINT,\n",
    "    host_url STRING,\n",
    "    host_name STRING,\n",
    "    host_since STRING,\n",
    "    host_location STRING,\n",
    "    host_about STRING,\n",
    "    host_response_time STRING,\n",
    "    host_response_rate STRING,\n",
    "    host_acceptance_rate STRING,\n",
    "    host_is_superhost STRING,\n",
    "    host_thumbnail_url STRING,\n",
    "    host_picture_url STRING,\n",
    "    host_neighbourhood STRING,\n",
    "    host_listings_count DOUBLE,\n",
    "    host_total_listings_count DOUBLE,\n",
    "    host_verifications STRING,\n",
    "    host_has_profile_pic STRING,\n",
    "    host_identity_verified STRING,\n",
    "    neighbourhood STRING,\n",
    "    neighbourhood_cleansed STRING,\n",
    "    neighbourhood_group_cleansed DOUBLE,\n",
    "    latitude DOUBLE,\n",
    "    longitude DOUBLE,\n",
    "    property_type STRING,\n",
    "    room_type STRING,\n",
    "    accommodates BIGINT,\n",
    "    bathrooms DOUBLE,\n",
    "    bathrooms_text STRING,\n",
    "    bedrooms DOUBLE,\n",
    "    beds DOUBLE,\n",
    "    amenities STRING,\n",
    "    price STRING,\n",
    "    minimum_nights BIGINT,\n",
    "    maximum_nights BIGINT,\n",
    "    minimum_minimum_nights DOUBLE,\n",
    "    maximum_minimum_nights DOUBLE,\n",
    "    minimum_maximum_nights DOUBLE,\n",
    "    maximum_maximum_nights DOUBLE,\n",
    "    minimum_nights_avg_ntm DOUBLE,\n",
    "    maximum_nights_avg_ntm DOUBLE,\n",
    "    calendar_updated DOUBLE,\n",
    "    has_availability STRING,\n",
    "    availability_30 BIGINT,\n",
    "    availability_60 BIGINT,\n",
    "    availability_90 BIGINT,\n",
    "    availability_365 BIGINT,\n",
    "    calendar_last_scraped STRING,\n",
    "    number_of_reviews BIGINT,\n",
    "    number_of_reviews_ltm BIGINT,\n",
    "    number_of_reviews_l30d BIGINT,\n",
    "    availability_eoy BIGINT,\n",
    "    number_of_reviews_ly BIGINT,\n",
    "    estimated_occupancy_l365d BIGINT,\n",
    "    estimated_revenue_l365d DOUBLE,\n",
    "    first_review STRING,\n",
    "    last_review STRING,\n",
    "    review_scores_rating DOUBLE,\n",
    "    review_scores_accuracy DOUBLE,\n",
    "    review_scores_cleanliness DOUBLE,\n",
    "    review_scores_checkin DOUBLE,\n",
    "    review_scores_communication DOUBLE,\n",
    "    review_scores_location DOUBLE,\n",
    "    review_scores_value DOUBLE,\n",
    "    license STRING,\n",
    "    instant_bookable STRING,\n",
    "    calculated_host_listings_count BIGINT,\n",
    "    calculated_host_listings_count_entire_homes BIGINT,\n",
    "    calculated_host_listings_count_private_rooms BIGINT,\n",
    "    calculated_host_listings_count_shared_rooms BIGINT,\n",
    "    reviews_per_month DOUBLE\n",
    ")\n",
    "-- city and extraction_date move here!\n",
    "PARTITIONED BY (extraction_date DATE, city STRING) \n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/hive/warehouse/airbnb.db/bronze/listings';\n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fe5625-370c-4c84-b214-3a6ff0307b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Register the partitions (Crucial!)\n",
    "spark.sql(\"MSCK REPAIR TABLE airbnb_bronze.listings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
