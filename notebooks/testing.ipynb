{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f1d3b6-aea5-4258-8bd3-b9d575448c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Testing Hive...\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  test_db|\n",
      "+---------+\n",
      "\n",
      "✅ Hive Metastore is connected!\n",
      "\n",
      "2. Testing ClickHouse Driver...\n",
      "✅ ClickHouse Driver is active!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, current_timestamp, concat, lit\n",
    "import os\n",
    "\n",
    "# Ensure we use the root user to avoid HDFS permission errors\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive-to-ClickHouse-Pipeline\") \\\n",
    "    .config(\"spark.driver.host\", \"spark-notebook\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- VERIFICATION ---\n",
    "print(\"1. Testing Hive...\")\n",
    "try:\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    print(\"✅ Hive Metastore is connected!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Hive Error: {e}\")\n",
    "\n",
    "print(\"\\n2. Testing ClickHouse Driver...\")\n",
    "try:\n",
    "    spark._jvm.Class.forName(\"com.clickhouse.jdbc.ClickHouseDriver\")\n",
    "    print(\"✅ ClickHouse Driver is active!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ClickHouse Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b68bd5-29d1-4831-9a80-148989d271eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random data...\n",
      "Attempting write with Direct URL credentials...\n",
      "❌ Write failed again. Let's check the error code one more time:\n",
      "An error occurred while calling o163.jdbc.\n",
      ": java.sql.SQLException: Code: 516. DB::Exception: default: Authentication failed: password is incorrect, or there is no user with such name. If you have installed ClickHouse and forgot password you can reset it in the configuration file. The password for default user is typically located at /etc/clickhouse-server/users.d/default-password.xml and deleting this file will reset the password. See also /etc/clickhouse-server/users.xml on the server where ClickHouse is installed. . (AUTHENTICATION_FAILED) (version 24.8.14.39 (official build)) \n",
      "\tat com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:67)\n",
      "\tat com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:42)\n",
      "\tat com.clickhouse.jdbc.StatementImpl.executeUpdateImpl(StatementImpl.java:225)\n",
      "\tat com.clickhouse.jdbc.StatementImpl.executeLargeUpdate(StatementImpl.java:620)\n",
      "\tat com.clickhouse.jdbc.StatementImpl.executeUpdate(StatementImpl.java:196)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:921)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: com.clickhouse.client.api.ServerException: Code: 516. DB::Exception: default: Authentication failed: password is incorrect, or there is no user with such name. If you have installed ClickHouse and forgot password you can reset it in the configuration file. The password for default user is typically located at /etc/clickhouse-server/users.d/default-password.xml and deleting this file will reset the password. See also /etc/clickhouse-server/users.xml on the server where ClickHouse is installed. . (AUTHENTICATION_FAILED) (version 24.8.14.39 (official build)) \n",
      "\tat com.clickhouse.client.api.internal.HttpAPIClientHelper.readError(HttpAPIClientHelper.java:403)\n",
      "\tat com.clickhouse.client.api.internal.HttpAPIClientHelper.executeRequest(HttpAPIClientHelper.java:467)\n",
      "\tat com.clickhouse.client.api.Client.lambda$query$5(Client.java:1599)\n",
      "\tat com.clickhouse.client.api.Client.runAsyncOperation(Client.java:2012)\n",
      "\tat com.clickhouse.client.api.Client.query(Client.java:1644)\n",
      "\tat com.clickhouse.client.api.Client.query(Client.java:1542)\n",
      "\tat com.clickhouse.jdbc.StatementImpl.executeUpdateImpl(StatementImpl.java:219)\n",
      "\t... 47 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate Random Data (10,000 rows)\n",
    "print(\"Generating random data...\")\n",
    "df = spark.range(0, 10000) \\\n",
    "    .withColumn(\"sensor_id\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"reading_value\", rand() * 50.0) \\\n",
    "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"status\", concat(lit(\"Status_\"), (rand() * 5).cast(\"int\")))\n",
    "\n",
    "ch_url = \"jdbc:ch://default:@analytics-clickhouse:8123/default\"\n",
    "\n",
    "# 2. We keep ONLY the driver and table options in the properties\n",
    "ch_properties = {\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "    \"createTableOptions\": \"ENGINE = MergeTree() ORDER BY (timestamp, sensor_id)\"\n",
    "}\n",
    "\n",
    "print(\"Attempting write with Direct URL credentials...\")\n",
    "try:\n",
    "    df.write.jdbc(\n",
    "        url=ch_url, \n",
    "        table=\"random_sensor_data\", \n",
    "        mode=\"overwrite\", \n",
    "        properties=ch_properties\n",
    "    )\n",
    "    print(\"✅ Success! Data written to ClickHouse.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Write failed again. Let's check the error code one more time:\\n{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
